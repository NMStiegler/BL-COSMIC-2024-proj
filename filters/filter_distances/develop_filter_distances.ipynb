{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Import useful packages\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "import socket\n",
    "from datetime import datetime, timedelta\n",
    "import multiprocessing\n",
    "from scipy.sparse import csr_array\n",
    "from scipy.sparse import lil_array\n",
    "from scipy.sparse import save_npz\n",
    "from scipy.sparse import load_npz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get paths to data\n",
    "hostname = socket.gethostname()\n",
    "\n",
    "if hostname == \"blpc1\" or hostname == \"blpc2\":\n",
    "    data_path = \"/datax/scratch/nstieg/\"\n",
    "elif hostname == \"cosmic-gpu-1\":\n",
    "    data_path = \"/mnt/cosmic-gpu-1/data0/nstiegle/\"\n",
    "else:\n",
    "    raise Exception(\"Data path not known\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Develop filter for hits within _hz of another hit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "full_dataset_path = data_path + \"25GHz_higher.pkl\"\n",
    "coherent_dataset_path = data_path + \"25GHz_higher_coherent.pkl\"\n",
    "incoherent_dataset_path = data_path + \"25GHz_higher_incoherent.pkl\"\n",
    "coherent_after_1_path = data_path + \"25GHz_higher_coherent_post_filter1.pkl\"\n",
    "coherent_after_2_path = data_path + \"25GHz_higher_coherent_post_filter2.pkl\"\n",
    "coherent_after_3_path = data_path + \"25GHz_higher_coherent_post_filter3.pkl\"\n",
    "\n",
    "# Read in data\n",
    "coherent = pd.read_pickle(coherent_dataset_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read in adjacency information\n",
    "adjacency_path = os.path.expanduser(\"~\") + \"/BL-COSMIC-2024-proj/frequency_adjacency/adjacent_in_coherent/\"\n",
    "distances_path = adjacency_path + \"coherent_within_1000hz.distances.npz\"\n",
    "mask_path = adjacency_path + \"coherent_within_1000hz.mask.npz\"\n",
    "distances = load_npz(distances_path)\n",
    "mask = load_npz(mask_path)\n",
    "assert(distances.shape[0] == coherent.shape[0])\n",
    "assert(mask.shape[0] == coherent.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Let's look at how many collisions there were within a threshold\n",
    "# Want to do (distances <= threshold) & mask to get all the values which are under the threshold\n",
    "# including zeroes which are let in by the mask\n",
    "# However, if we turn all the 0s in the sparse array to Trues, it's not sparse anymore (so it's super slow)\n",
    "# So how do we maintain the sparsity?\n",
    "# Well, we can look for the opposite first, the values which are outside the threshold\n",
    "# Then we can remove those from the mask so there's a new mask just contains the values we care about below the threshold \n",
    "# Then the number of collisions within that threshold is the number of Trues in the new mask\n",
    "# And if we want to get those distances (or the hits which are close to each other), then we can do distances[new_mask] (or new_mask.nonzero())\n",
    "def find_collisions_at_threshold(threshold):\n",
    "    # Get those outside threshold\n",
    "    outside_threshold = distances > threshold\n",
    "\n",
    "    # Remove those outside threshold from the mask\n",
    "    # We want to do mask & (~outside_threshold), but note that ~outside_threshold produces an array which is mostly true\n",
    "    # So instead we'll have to do mask - outside_threshold \n",
    "    # (which does xor, so it will have values which are in mask but not outside_threshold, and those in outside_threshold but not mask)\n",
    "    # Finally, to get rid of values in outside_threshold but not mask, we'll and by mask\n",
    "    new_mask = mask.multiply(mask - outside_threshold)\n",
    "\n",
    "    return new_mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find all hits which are within 2hz of another hit\n",
    "within_2_hz = find_collisions_at_threshold(2e-6)\n",
    "print(within_2_hz.sum(), \"collisions\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the indices of those hits\n",
    "indices_of_hits_within_2hz = np.unique(np.concatenate(within_2_hz.nonzero()))\n",
    "print(len(indices_of_hits_within_2hz))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2262302\n"
     ]
    }
   ],
   "source": [
    "# Make a df which is all but those hits\n",
    "\n",
    "# Get a list of booleans which are true for hits which have another hit within 2hz\n",
    "hit_with_close_neighbor = np.zeros(len(coherent), dtype=bool)\n",
    "hit_with_close_neighbor[indices_of_hits_within_2hz] = True\n",
    "\n",
    "# Get list of booleans which are true for hits with no other hits within 2hz of them\n",
    "hits_with_no_close_neighbor = ~hit_with_close_neighbor\n",
    "\n",
    "# Then index the coherent dataframe at those locations\n",
    "lonely_hits = coherent.iloc[hits_with_no_close_neighbor]\n",
    "\n",
    "print(len(lonely_hits))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get datasets of just the data that had a collision or didn't\n",
    "indices_of_hits_with_collisions = np.unique(np.concatenate(hit_indices))\n",
    "bool_indices_of_hits_with_collisions = np.zeros(len(first_source), dtype=bool)\n",
    "bool_indices_of_hits_with_collisions[indices_of_hits_with_collisions] = True\n",
    "zero_distance_dataset = first_source.iloc[bool_indices_of_hits_with_collisions]\n",
    "non_zero_distance_dataset = first_source.iloc[~bool_indices_of_hits_with_collisions]\n",
    "print(f\"Collisions dataset: {zero_distance_dataset.shape}\")\n",
    "print(f\"No Collisions dataset: {non_zero_distance_dataset.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Look for hits which drifted from a previous hit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from astropy.time import Time\n",
    "\n",
    "# # Get the coherent dataset but with all the columns\n",
    "# full_dataset_path = data_path + \"25GHz_higher.pkl\"\n",
    "# df = pd.read_pickle(full_dataset_path)\n",
    "# full_coherent = df[(df.source_name != \"Incoherent\") & (df.source_name != \"PHASE_CENTER\")].copy()\n",
    "# full_coherent[\"tstart_h\"] = Time(full_coherent[\"tstart\"], format=\"mjd\").datetime\n",
    "# full_coherent.to_pickle(data_path + \"25GHz_higher_coherent_all_columns.pkl\")\n",
    "\n",
    "# Read in coheren dataset with all columns\n",
    "full_coherent = pd.read_pickle(data_path + \"25GHz_higher_coherent_all_columns.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pass in row of dataframe for a single hit, get the error on that drift rate\n",
    "def sigma_drift_rate(hit):\n",
    "    # Error propagation on the error of the drift rate as dr =  df/dt (change in frequency / change in time)\n",
    "    signal_dt = hit.tsamp * hit.signal_num_timesteps # Total number of seconds observed for\n",
    "    signal_dr = hit.signal_drift_rate # Drift rate observed\n",
    "    sigma_df = 2 # Error in measured frequency - 2Hz bins\n",
    "    sigma_dt = hit.tsamp # Error in measured time - tsamp integration time per timestep\n",
    "    return abs((signal_dr / signal_dt) * np.sqrt((sigma_df/ signal_dr)**2 + (sigma_dt)**2)) # Error propagation formula for division substituting df = dr * dt\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The first hit is of source 2535280716217508992 at 33262.648879MHz with drift rate -0.243Hz/s at 2023-12-29 03:49:37.462265\n",
      "The next time is 58.720261 seconds laer\n",
      "So 58.720261 seconds drifting at -0.243 +/- 0.239Hz/s gives -14.242 +/- 14.028Hz expected drift\n",
      "So there is/are 1 candidate(s) within +/- 14.028Hz of the target frequency of original_frequency + -14.242Hz\n",
      "The closest candidate was 10.427 Hz off, -3.815Hz from the original hit\n"
     ]
    }
   ],
   "source": [
    "# So if a hit has a drift rate, we might expect to see another signal that far away from it\n",
    "# if there's a close re-observation. Let's see if we can look for that\n",
    "\n",
    "# So we should group by source \n",
    "sources = full_coherent.groupby('source_name')\n",
    "source_names = list(sources.groups.keys())\n",
    "\n",
    "# We'll just look at the first source\n",
    "source = sources.get_group(source_names[0])\n",
    "\n",
    "# And then let's group by time\n",
    "times = source.groupby('tstart_h')\n",
    "all_times = list(times.groups.keys())\n",
    "\n",
    "# Then let's look at the first hit in that time\n",
    "first_time = times.get_group(all_times[0])\n",
    "hit = first_time.iloc[0]\n",
    "\n",
    "# Some info about that hit\n",
    "f = hit.signal_frequency\n",
    "dr = hit.signal_drift_rate\n",
    "dr_mhz = dr * 1e-6\n",
    "source_name = hit.source_name\n",
    "time = hit.tstart_h\n",
    "print(f\"The first hit is of source {source_name} at {round(f, 6)}MHz with drift rate {round(dr, 3)}Hz/s at {time}\")\n",
    "\n",
    "# Now let's look in the next time to see if there's a signal within that drift rate\n",
    "dt_h = all_times[1] - all_times[0]\n",
    "dt = dt_h.total_seconds()\n",
    "print(f\"The next time is {dt} seconds laer\")\n",
    "dt = dt_h.total_seconds()\n",
    "sigma_dr_hz = sigma_drift_rate(hit)\n",
    "drift_hz = dt * dr\n",
    "drift_mhz = dt * dr_mhz\n",
    "sigma_drift_hz = dt * sigma_dr_hz\n",
    "print(f\"So {dt} seconds drifting at {round(dr, 3)} +/- {round(sigma_dr_hz, 3)}Hz/s gives {round(drift_hz, 3)} +/- {round(sigma_drift_hz, 3)}Hz expected drift\")\n",
    "\n",
    "target_f = f + (drift_hz * 1e-6)\n",
    "second_time = times.get_group(all_times[1])\n",
    "ebar_mhz = max(dt * sigma_drift_rate(hit) * 1e-6, 2 * 1e-6) # Mhz\n",
    "candidates = second_time[(second_time.signal_frequency > target_f - ebar_mhz) & (second_time.signal_frequency < target_f + ebar_mhz)]\n",
    "print(f\"So there is/are {len(candidates)} candidate(s) within +/- {round(ebar_mhz * 1e6, 3)}Hz of the target frequency of original_frequency + {round(drift_hz, 3)}Hz\")\n",
    "closest_candidates = candidates.copy().sort_values(\"signal_frequency\").reset_index(drop=True)\n",
    "closest_candidate = closest_candidates.iloc[0]\n",
    "print(f\"The closest candidate was {round((closest_candidate.signal_frequency - target_f) * 1e6, 3)} Hz off, {round((closest_candidate.signal_frequency - f) * 1e6, 3)}Hz from the original hit\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TOOD / FUTURE WORK\n",
    "# - Figure out what the distribution of dts are for sources\n",
    "# - The results of this search approach are naturally described by a directed graph (or collection of trees)\n",
    "#   where nodes are hits and hits point to hits they may have drifted to. By following 'chains' in this directed\n",
    "#   graph you might find hits which continue to drift for some time or change drift rate (maybe sinusoidally?)\n",
    "\n",
    "# Parameters of search\n",
    "max_drift_time_to_search = 10 * 60 # in seconds\n",
    "\n",
    "# Setup dataframe to flag hits which are validated by search\n",
    "full_coherent[\"valid\"] = False\n",
    "\n",
    "# Do search within each source\n",
    "for source_name, source_group in full_coherent.groupby('source_name'):\n",
    "    # Group by time and figure out what all the times observed are\n",
    "    time_groups = source_group.groupby('tstart_h')\n",
    "    time_names = list(time_groups.groups.keys())\n",
    "\n",
    "    # Look at all times for this source which have a following time (all but the last)\n",
    "    for t_idx in range(0, len(time_names) - 1):\n",
    "        this_time = time_names[t_idx]\n",
    "        next_time = time_names[t_idx + 1]\n",
    "        dt = (next_time - this_time).total_seconds()\n",
    "\n",
    "        # If the source was observed again within 10 minutes, look for\n",
    "        # signals which drifted in the next observation time\n",
    "        if dt <= max_drift_time_to_search:\n",
    "            time_group = time_groups.get_group(this_time)\n",
    "            next_time_group = time_groups.get_group(next_time)\n",
    "            for i, hit in time_group.iterrows():\n",
    "                # Ignore zero drift rate signals\n",
    "                if hit.signal_drift_rate != 0:\n",
    "                    # Compute some useful quantities\n",
    "                    drift = (dt * hit.signal_drift_rate) * 1e-6 # Total drift in MHz\n",
    "                    sigma_drift = max((dt * sigma_drift_rate(hit)) * 1e-6, 2 * 1e-6) # Error in drift in Mhz\n",
    "                    expected_new_frequency = hit.signal_frequency + drift # Where we expect it to drift to in Mhz\n",
    "\n",
    "                    # Get candidate hits from the next time\n",
    "                    candidates = next_time_group[(next_time_group.signal_frequency > expected_new_frequency - sigma_drift) &\n",
    "                                                (next_time_group.signal_frequency < expected_new_frequency + sigma_drift)]\n",
    "                    \n",
    "                    # If there was a match between this hit and a hit in the target range, validate them both in the full dataset\n",
    "                    full_coherent.loc[full_coherent.id == hit.id, 'valid'] = True\n",
    "                    full_coherent.loc[candidates.index, 'valid'] = True\n",
    "\n",
    "results = full_coherent[\"id\"][full_coherent[\"valid\"]]\n",
    "np.save(\"/home/nstieg/BL-COSMIC-2024-proj/filters/filter11/run_filter_11_coherent_results.npy\", results.values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cosmic",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
